{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b73d7d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision.datasets import ImageFolder\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import cv2\n",
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "import copy\n",
    "import time\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data.dataset import random_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73083aaa",
   "metadata": {},
   "source": [
    "# Dataset 과 DataLoader 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f4c40a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "class custom_dataset(Dataset):\n",
    "    \n",
    "    def __init__(self, inputs_dir, transform = None):\n",
    "        \n",
    "        self.inputs_dir = inputs_dir \n",
    "        self.inputs_list =  os.listdir(inputs_dir)\n",
    "                \n",
    "        self.transform = transform\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.inputs_list)\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "    \n",
    "        os.chdir(self.inputs_dir)\n",
    "        target_label = self.inputs_list[idx][-5]\n",
    "        target_label = int(target_label)\n",
    "        # 이미 numpy도 변환하여 저장 해 두었다.\n",
    "        input_image_numpy = np.load(self.inputs_list[idx])  \n",
    "        input_image_numpy = cv2.resize(input_image_numpy, (256,256), interpolation = cv2.INTER_LANCZOS4) \n",
    "        \n",
    "        combine = {'input':input_image_numpy, 'target':target_label}  # segmenatation과 통일하기 위해 합치지만 굳이 합칠 필요는 없고\n",
    "        # 심지어 custom transformation에서 아예 화용하지도 않는다. \n",
    "        # 다만 메모리의 차지, time cost등이 있지만 무시할 정도여서 그냥 이렇게 한다.\n",
    "        \n",
    "        if self.transform:\n",
    "            combine = self.transform(combine)\n",
    "            \n",
    "        input_tensor = torchvision.transforms.functional.to_tensor(combine['input'])\n",
    "        target_tensor = torch.tensor(combine['target'])\n",
    "\n",
    "\n",
    "        return (input_tensor , target_tensor)\n",
    "    \n",
    "class RandomFlip(object):\n",
    "    # input으로 numpy를 받는다.\n",
    "    \n",
    "    def __init__(self, horizontal = True, vertical = False, p = 0.5): \n",
    "        self.horizontal = horizontal\n",
    "        self.vertical = vertical\n",
    "        self.p = p # p는 그냥 예의상 넣었다. 건들이는 경우가 있나 싶긴하다\n",
    "        \n",
    "        \n",
    "    def __call__(self, combine):\n",
    "    \n",
    "        inputs = combine['input']   # (224, 224, 3)\n",
    "        targets = combine['target']  # target은 건들이지 않는다\n",
    "\n",
    "        if (self.horizontal) and (np.random.rand() > self.p):\n",
    "            inputs = cv2.flip(inputs,1)\n",
    "        \n",
    "        if (self.vertical) and (np.random.rand() > self.p):\n",
    "            inputs = cv2.flip(inputs,0)\n",
    "\n",
    "\n",
    "        combine = {'input': inputs, 'target': targets}  #출력은 tensor\n",
    "\n",
    "        return combine\n",
    "\n",
    "path_train_inputs = '/home/mskang/hyeokjong/cancer/2019/npy_aug'\n",
    "\n",
    "transformation = transforms.Compose([ RandomFlip(True, True, 0.5)])\n",
    "\n",
    "dataset = custom_dataset(path_train_inputs, transformation)\n",
    "\n",
    "train_size = int(0.7*len(dataset))\n",
    "v_size = len(dataset) - train_size\n",
    "train_dataset, v_dataset = random_split(dataset, [train_size, v_size])\n",
    "val_size = int(0.5*len(v_dataset))\n",
    "test_size = len(v_dataset) - val_size\n",
    "val_dataset, test_dataset = random_split(v_dataset, [val_size, test_size])\n",
    "\n",
    "\n",
    "batch_size = 64\n",
    "train_dl = DataLoader(train_dataset, batch_size, shuffle = True, num_workers = 4, pin_memory = True)\n",
    "val_dl = DataLoader(val_dataset, batch_size, shuffle = True, num_workers = 4, pin_memory = True)\n",
    "test_dl = DataLoader(test_dataset, batch_size, shuffle = False, num_workers = 4, pin_memory = True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hyeokjong2",
   "language": "python",
   "name": "hyeikjong2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
